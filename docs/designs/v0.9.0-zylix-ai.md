# Zylix AI - 組み込みAI設計書 (v0.9.0)

> **ステータス**: 計画中
> **予定リリース**: 2026年Q1
> **前提条件**: v0.8.x 完了

---

## 1. 概要

### 1.1 目的

オンデバイスでのLLM/VLM推論を実現し、プライバシー保護とオフライン動作を可能にする組み込みAI機能を提供する。

### 1.2 設計原則

1. **プライバシーファースト**: データはデバイス内で処理、外部送信なし
2. **オフライン動作**: ネットワーク不要で全機能動作
3. **リソース効率**: モバイルデバイスでも動作する軽量設計
4. **プラットフォーム最適化**: 各プラットフォームのハードウェアアクセラレーション活用
5. **モデル非同梱**: Zylixはモデルを配布しない。エンドユーザーが自己責任で入手・配置する

---

## 2. アーキテクチャ

### 2.1 レイヤー構成

```
┌─────────────────────────────────────────────────────────────┐
│                    Zylix AI Public API                      │
│  (ZylixEmbedding, ZylixLLM, ZylixVLM, ZylixWhisper)        │
├─────────────────────────────────────────────────────────────┤
│                    Model Manager                            │
│  (モデルロード、キャッシュ、量子化、メモリ管理)              │
├─────────────────────────────────────────────────────────────┤
│                  Inference Engine                           │
│  (推論実行、バッチ処理、ストリーミング出力)                  │
├─────────────────────────────────────────────────────────────┤
│              Platform Backend Abstraction                   │
├──────────┬──────────┬──────────┬──────────┬────────────────┤
│  iOS     │ Android  │   Web    │  macOS   │ Windows/Linux  │
│ Core ML  │ NNAPI    │ WebGPU   │  Metal   │ GGML/ONNX      │
│ Metal    │ TFLite   │ ONNX.js  │ Core ML  │ Vulkan/CUDA    │
└──────────┴──────────┴──────────┴──────────┴────────────────┘
```

### 2.2 コアコンポーネント

```zig
// core/src/ai/types.zig

/// AIモデルの種類
pub const ModelType = enum {
    embedding,      // テキスト埋め込み
    llm,            // 言語モデル
    vlm,            // ビジョン言語モデル
    whisper,        // 音声認識
    tts,            // テキスト読み上げ
};

/// モデル設定
pub const ModelConfig = struct {
    model_type: ModelType,
    model_path: []const u8,

    // 量子化設定
    quantization: Quantization = .q4_0,

    // メモリ設定
    max_memory_mb: u32 = 512,
    use_mmap: bool = true,

    // 推論設定
    context_length: u32 = 2048,
    batch_size: u32 = 1,
    num_threads: u8 = 4,

    // ハードウェアアクセラレーション
    use_gpu: bool = true,
    gpu_layers: u32 = 0,  // 0 = 自動
};

/// 量子化レベル
pub const Quantization = enum {
    f16,    // 16-bit float (高品質、大メモリ)
    q8_0,   // 8-bit (バランス)
    q4_0,   // 4-bit (軽量、推奨)
    q4_k_m, // 4-bit K-quant (最適バランス)
    q2_k,   // 2-bit (超軽量、品質低下)
};
```

---

## 3. 埋め込みモデル (Embedding)

### 3.1 サポートモデル

| モデル | パラメータ | サイズ (Q4) | 用途 |
|--------|-----------|-------------|------|
| Qwen3-Embedding-0.6B | 600M | ~350MB | セマンティック検索 |
| all-MiniLM-L6-v2 | 22M | ~25MB | 軽量埋め込み |
| bge-small-en-v1.5 | 33M | ~35MB | 英語特化 |
| multilingual-e5-small | 118M | ~70MB | 多言語対応 |

### 3.2 API設計

```zig
// core/src/ai/embedding.zig

pub const EmbeddingModel = struct {
    config: ModelConfig,
    backend: *Backend,

    const Self = @This();

    /// モデルをロード
    pub fn load(config: ModelConfig) !Self {
        const backend = try Backend.init(config);
        return Self{
            .config = config,
            .backend = backend,
        };
    }

    /// テキストを埋め込みベクトルに変換
    pub fn embed(self: *Self, text: []const u8) ![]f32 {
        return self.backend.runEmbedding(text);
    }

    /// バッチ埋め込み
    pub fn embedBatch(self: *Self, texts: []const []const u8) ![][]f32 {
        return self.backend.runEmbeddingBatch(texts);
    }

    /// コサイン類似度計算
    pub fn similarity(a: []const f32, b: []const f32) f32 {
        var dot: f32 = 0;
        var norm_a: f32 = 0;
        var norm_b: f32 = 0;

        for (a, b) |va, vb| {
            dot += va * vb;
            norm_a += va * va;
            norm_b += vb * vb;
        }

        return dot / (@sqrt(norm_a) * @sqrt(norm_b));
    }

    pub fn deinit(self: *Self) void {
        self.backend.deinit();
    }
};
```

### 3.3 TypeScript API

```typescript
// bindings/typescript/src/ai/embedding.ts

export interface EmbeddingConfig {
  modelPath: string;
  quantization?: 'f16' | 'q8_0' | 'q4_0' | 'q4_k_m';
  maxMemoryMB?: number;
  useGPU?: boolean;
}

export class ZylixEmbedding {
  private handle: number;

  static async load(config: EmbeddingConfig): Promise<ZylixEmbedding> {
    const handle = await native.loadEmbeddingModel(config);
    return new ZylixEmbedding(handle);
  }

  async embed(text: string): Promise<Float32Array> {
    return native.embed(this.handle, text);
  }

  async embedBatch(texts: string[]): Promise<Float32Array[]> {
    return native.embedBatch(this.handle, texts);
  }

  static cosineSimilarity(a: Float32Array, b: Float32Array): number {
    let dot = 0, normA = 0, normB = 0;
    for (let i = 0; i < a.length; i++) {
      dot += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }
    return dot / (Math.sqrt(normA) * Math.sqrt(normB));
  }

  async dispose(): Promise<void> {
    await native.unloadModel(this.handle);
  }
}
```

### 3.4 使用例: セマンティック検索

```typescript
// ドキュメント検索システムの例

import { ZylixEmbedding } from '@zylix/ai';

async function semanticSearch() {
  // モデルロード
  const embedding = await ZylixEmbedding.load({
    modelPath: 'models/qwen3-embedding-0.6b-q4.gguf',
    useGPU: true,
  });

  // ドキュメントの埋め込み
  const documents = [
    "Zylixはクロスプラットフォームフレームワークです",
    "Rustは安全性を重視したプログラミング言語です",
    "機械学習は人工知能の一分野です",
  ];

  const docEmbeddings = await embedding.embedBatch(documents);

  // クエリ検索
  const query = "UIフレームワークについて";
  const queryEmbedding = await embedding.embed(query);

  // 類似度計算とランキング
  const results = documents
    .map((doc, i) => ({
      document: doc,
      score: ZylixEmbedding.cosineSimilarity(queryEmbedding, docEmbeddings[i]),
    }))
    .sort((a, b) => b.score - a.score);

  console.log('検索結果:', results);
  // => [{ document: "Zylixは...", score: 0.85 }, ...]

  await embedding.dispose();
}
```

---

## 4. 言語モデル (LLM)

### 4.1 サポートモデル

| モデル | パラメータ | サイズ (Q4) | VRAM/RAM | 特徴 |
|--------|-----------|-------------|----------|------|
| Qwen3-0.6B | 600M | ~400MB | 1GB | 超軽量 |
| Qwen3-1.7B | 1.7B | ~1GB | 2GB | バランス |
| Qwen3-4B | 4B | ~2.5GB | 4GB | 高品質 |
| Phi-4-mini | 3.8B | ~2.3GB | 4GB | 推論特化 |
| Gemma-2B | 2B | ~1.3GB | 2.5GB | Google製 |
| Llama-3.2-1B | 1B | ~700MB | 1.5GB | Meta製 |
| Llama-3.2-3B | 3B | ~2GB | 3GB | Meta製 |

### 4.2 API設計

```zig
// core/src/ai/llm.zig

pub const LLMModel = struct {
    config: ModelConfig,
    backend: *Backend,
    context: *Context,

    const Self = @This();

    pub fn load(config: ModelConfig) !Self {
        const backend = try Backend.init(config);
        const context = try Context.init(config.context_length);
        return Self{
            .config = config,
            .backend = backend,
            .context = context,
        };
    }

    /// テキスト生成 (同期)
    pub fn generate(self: *Self, prompt: []const u8, params: GenerateParams) ![]const u8 {
        return self.backend.generate(prompt, params);
    }

    /// ストリーミング生成
    pub fn generateStream(
        self: *Self,
        prompt: []const u8,
        params: GenerateParams,
        callback: *const fn (token: []const u8) void,
    ) !void {
        return self.backend.generateStream(prompt, params, callback);
    }

    /// チャット形式の生成
    pub fn chat(self: *Self, messages: []const ChatMessage, params: GenerateParams) ![]const u8 {
        const formatted = self.formatChat(messages);
        return self.generate(formatted, params);
    }

    pub fn deinit(self: *Self) void {
        self.context.deinit();
        self.backend.deinit();
    }
};

pub const GenerateParams = struct {
    max_tokens: u32 = 256,
    temperature: f32 = 0.7,
    top_p: f32 = 0.9,
    top_k: u32 = 40,
    repeat_penalty: f32 = 1.1,
    stop_sequences: []const []const u8 = &.{},
};

pub const ChatMessage = struct {
    role: Role,
    content: []const u8,

    pub const Role = enum { system, user, assistant };
};
```

### 4.3 TypeScript API

```typescript
// bindings/typescript/src/ai/llm.ts

export interface LLMConfig {
  modelPath: string;
  contextLength?: number;
  quantization?: 'f16' | 'q8_0' | 'q4_0' | 'q4_k_m';
  maxMemoryMB?: number;
  useGPU?: boolean;
  gpuLayers?: number;
}

export interface GenerateParams {
  maxTokens?: number;
  temperature?: number;
  topP?: number;
  topK?: number;
  repeatPenalty?: number;
  stopSequences?: string[];
}

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export class ZylixLLM {
  private handle: number;

  static async load(config: LLMConfig): Promise<ZylixLLM> {
    const handle = await native.loadLLMModel(config);
    return new ZylixLLM(handle);
  }

  // 単発生成
  async generate(prompt: string, params?: GenerateParams): Promise<string> {
    return native.generate(this.handle, prompt, params ?? {});
  }

  // ストリーミング生成
  async *generateStream(
    prompt: string,
    params?: GenerateParams
  ): AsyncGenerator<string, void, unknown> {
    const stream = native.generateStream(this.handle, prompt, params ?? {});
    for await (const token of stream) {
      yield token;
    }
  }

  // チャット形式
  async chat(messages: ChatMessage[], params?: GenerateParams): Promise<string> {
    return native.chat(this.handle, messages, params ?? {});
  }

  // チャットストリーミング
  async *chatStream(
    messages: ChatMessage[],
    params?: GenerateParams
  ): AsyncGenerator<string, void, unknown> {
    const stream = native.chatStream(this.handle, messages, params ?? {});
    for await (const token of stream) {
      yield token;
    }
  }

  async dispose(): Promise<void> {
    await native.unloadModel(this.handle);
  }
}
```

### 4.4 使用例: チャットボット

```typescript
import { ZylixLLM, ChatMessage } from '@zylix/ai';

async function chatExample() {
  const llm = await ZylixLLM.load({
    modelPath: 'models/qwen3-1.7b-q4_k_m.gguf',
    contextLength: 4096,
    useGPU: true,
  });

  const messages: ChatMessage[] = [
    { role: 'system', content: 'あなたは親切なAIアシスタントです。' },
    { role: 'user', content: 'Zylixフレームワークの特徴を教えてください。' },
  ];

  // ストリーミングで回答を表示
  process.stdout.write('Assistant: ');
  for await (const token of llm.chatStream(messages, { maxTokens: 500 })) {
    process.stdout.write(token);
  }
  console.log();

  await llm.dispose();
}
```

---

## 5. ビジョン言語モデル (VLM)

### 5.1 サポートモデル

| モデル | パラメータ | サイズ (Q4) | 機能 |
|--------|-----------|-------------|------|
| Qwen2-VL-2B | 2B | ~1.3GB | 画像理解、OCR |
| LLaVA-1.6-7B | 7B | ~4GB | 高品質画像分析 |
| PaliGemma-3B | 3B | ~2GB | 多言語画像理解 |
| MiniCPM-V-2.6 | 2.6B | ~1.5GB | 軽量VLM |

### 5.2 API設計

```zig
// core/src/ai/vlm.zig

pub const VLMModel = struct {
    config: ModelConfig,
    backend: *Backend,
    image_processor: *ImageProcessor,

    const Self = @This();

    pub fn load(config: ModelConfig) !Self {
        const backend = try Backend.init(config);
        const image_processor = try ImageProcessor.init();
        return Self{
            .config = config,
            .backend = backend,
            .image_processor = image_processor,
        };
    }

    /// 画像を分析
    pub fn analyze(self: *Self, image: Image, prompt: []const u8) ![]const u8 {
        const processed = try self.image_processor.process(image);
        return self.backend.runVLM(processed, prompt);
    }

    /// 画像からテキスト抽出 (OCR)
    pub fn extractText(self: *Self, image: Image) ![]const u8 {
        return self.analyze(image, "この画像内のテキストをすべて抽出してください。");
    }

    /// 画像の説明生成
    pub fn describe(self: *Self, image: Image) ![]const u8 {
        return self.analyze(image, "この画像を詳細に説明してください。");
    }

    pub fn deinit(self: *Self) void {
        self.image_processor.deinit();
        self.backend.deinit();
    }
};

pub const Image = struct {
    data: []const u8,
    width: u32,
    height: u32,
    format: ImageFormat,

    pub const ImageFormat = enum { rgb, rgba, grayscale };
};
```

### 5.3 TypeScript API

```typescript
// bindings/typescript/src/ai/vlm.ts

export interface VLMConfig {
  modelPath: string;
  maxImageSize?: number;  // px
  useGPU?: boolean;
}

export class ZylixVLM {
  private handle: number;

  static async load(config: VLMConfig): Promise<ZylixVLM> {
    const handle = await native.loadVLMModel(config);
    return new ZylixVLM(handle);
  }

  // 画像分析
  async analyze(image: ImageSource, prompt: string): Promise<string> {
    const imageData = await this.processImage(image);
    return native.analyzeImage(this.handle, imageData, prompt);
  }

  // OCR
  async extractText(image: ImageSource): Promise<string> {
    return this.analyze(image, 'Extract all text from this image.');
  }

  // 画像説明
  async describe(image: ImageSource): Promise<string> {
    return this.analyze(image, 'Describe this image in detail.');
  }

  // 質問応答
  async ask(image: ImageSource, question: string): Promise<string> {
    return this.analyze(image, question);
  }

  private async processImage(source: ImageSource): Promise<Uint8Array> {
    // File, Blob, URL, base64, Canvas などを統一形式に変換
    // ...
  }

  async dispose(): Promise<void> {
    await native.unloadModel(this.handle);
  }
}

type ImageSource =
  | File
  | Blob
  | string  // URL or base64
  | HTMLCanvasElement
  | ImageData;
```

### 5.4 使用例: 画像分析アプリ

```typescript
import { ZylixVLM } from '@zylix/ai';

async function analyzeReceipt() {
  const vlm = await ZylixVLM.load({
    modelPath: 'models/qwen2-vl-2b-q4.gguf',
    useGPU: true,
  });

  // 領収書画像からテキスト抽出
  const receiptImage = document.getElementById('receipt') as HTMLImageElement;
  const text = await vlm.extractText(receiptImage);
  console.log('抽出テキスト:', text);

  // 構造化データとして解析
  const analysis = await vlm.analyze(
    receiptImage,
    '領収書から以下を抽出: 店舗名、日付、合計金額、各商品と価格。JSON形式で出力。'
  );
  console.log('解析結果:', JSON.parse(analysis));

  await vlm.dispose();
}
```

---

## 6. 音声認識 (Whisper)

### 6.1 サポートモデル

| モデル | パラメータ | サイズ (Q4) | 言語 | 速度 |
|--------|-----------|-------------|------|------|
| whisper-tiny | 39M | ~40MB | 多言語 | 超高速 |
| whisper-base | 74M | ~75MB | 多言語 | 高速 |
| whisper-small | 244M | ~150MB | 多言語 | 中速 |
| whisper-medium | 769M | ~450MB | 多言語 | 低速 |

### 6.2 API設計

```typescript
// bindings/typescript/src/ai/whisper.ts

export interface WhisperConfig {
  modelPath: string;
  language?: string;  // 'ja', 'en', 'auto'
  useGPU?: boolean;
}

export interface TranscriptionResult {
  text: string;
  segments: TranscriptionSegment[];
  language: string;
  duration: number;
}

export interface TranscriptionSegment {
  start: number;  // 秒
  end: number;
  text: string;
  confidence: number;
}

export class ZylixWhisper {
  private handle: number;

  static async load(config: WhisperConfig): Promise<ZylixWhisper> {
    const handle = await native.loadWhisperModel(config);
    return new ZylixWhisper(handle);
  }

  // 音声ファイルを文字起こし
  async transcribe(audio: AudioSource): Promise<TranscriptionResult> {
    const audioData = await this.processAudio(audio);
    return native.transcribe(this.handle, audioData);
  }

  // リアルタイム文字起こし
  async *transcribeStream(
    audioStream: ReadableStream<Float32Array>
  ): AsyncGenerator<TranscriptionSegment, void, unknown> {
    // ストリーミング処理
  }

  // 翻訳 (英語へ)
  async translate(audio: AudioSource): Promise<TranscriptionResult> {
    const audioData = await this.processAudio(audio);
    return native.translate(this.handle, audioData);
  }

  async dispose(): Promise<void> {
    await native.unloadModel(this.handle);
  }
}

type AudioSource = File | Blob | Float32Array | string;
```

### 6.3 使用例: 音声メモアプリ

```typescript
import { ZylixWhisper } from '@zylix/ai';

async function voiceMemo() {
  const whisper = await ZylixWhisper.load({
    modelPath: 'models/whisper-small-q4.bin',
    language: 'ja',
    useGPU: true,
  });

  // マイクから録音
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  const recorder = new MediaRecorder(stream);
  const chunks: Blob[] = [];

  recorder.ondataavailable = (e) => chunks.push(e.data);
  recorder.onstop = async () => {
    const audioBlob = new Blob(chunks, { type: 'audio/webm' });

    // 文字起こし
    const result = await whisper.transcribe(audioBlob);
    console.log('文字起こし:', result.text);
    console.log('セグメント:', result.segments);
  };

  // 5秒間録音
  recorder.start();
  setTimeout(() => recorder.stop(), 5000);
}
```

---

## 7. プラットフォーム実装

### 7.1 iOS (Core ML + Metal)

```swift
// platforms/ios/Sources/ZylixAI/CoreMLBackend.swift

import CoreML
import Metal

public class CoreMLBackend: AIBackend {
    private var model: MLModel?
    private let device: MTLDevice
    private let commandQueue: MTLCommandQueue

    public init() throws {
        guard let device = MTLCreateSystemDefaultDevice() else {
            throw AIError.gpuNotAvailable
        }
        self.device = device
        self.commandQueue = device.makeCommandQueue()!
    }

    public func loadModel(path: String, config: ModelConfig) throws {
        let compiledURL = try MLModel.compileModel(at: URL(fileURLWithPath: path))

        let configuration = MLModelConfiguration()
        configuration.computeUnits = config.useGPU ? .all : .cpuOnly

        self.model = try MLModel(contentsOf: compiledURL, configuration: configuration)
    }

    public func runInference(input: MLFeatureProvider) throws -> MLFeatureProvider {
        guard let model = self.model else {
            throw AIError.modelNotLoaded
        }
        return try model.prediction(from: input)
    }
}
```

### 7.2 Android (NNAPI + TensorFlow Lite)

```kotlin
// platforms/android/src/main/kotlin/com/zylix/ai/NNAPIBackend.kt

import org.tensorflow.lite.Interpreter
import org.tensorflow.lite.gpu.GpuDelegate

class NNAPIBackend : AIBackend {
    private var interpreter: Interpreter? = null
    private var gpuDelegate: GpuDelegate? = null

    override fun loadModel(path: String, config: ModelConfig) {
        val options = Interpreter.Options()

        if (config.useGPU) {
            gpuDelegate = GpuDelegate()
            options.addDelegate(gpuDelegate)
        }

        options.setNumThreads(config.numThreads)
        options.setUseNNAPI(true)

        interpreter = Interpreter(File(path), options)
    }

    override fun runInference(input: ByteBuffer): ByteBuffer {
        val output = ByteBuffer.allocateDirect(outputSize)
        interpreter?.run(input, output)
        return output
    }

    override fun close() {
        interpreter?.close()
        gpuDelegate?.close()
    }
}
```

### 7.3 Web (WebGPU + ONNX.js)

```typescript
// platforms/web/src/ai/webgpu-backend.ts

import * as ort from 'onnxruntime-web';

export class WebGPUBackend implements AIBackend {
  private session: ort.InferenceSession | null = null;

  async loadModel(path: string, config: ModelConfig): Promise<void> {
    const options: ort.InferenceSession.SessionOptions = {
      executionProviders: config.useGPU
        ? ['webgpu', 'wasm']
        : ['wasm'],
      graphOptimizationLevel: 'all',
    };

    this.session = await ort.InferenceSession.create(path, options);
  }

  async runInference(inputs: Record<string, ort.Tensor>): Promise<Record<string, ort.Tensor>> {
    if (!this.session) throw new Error('Model not loaded');
    return this.session.run(inputs);
  }

  async dispose(): Promise<void> {
    await this.session?.release();
  }
}
```

### 7.4 Desktop (GGML/llama.cpp)

```zig
// core/src/ai/backends/ggml_backend.zig

const llama = @cImport({
    @cInclude("llama.h");
});

pub const GGMLBackend = struct {
    model: *llama.llama_model,
    ctx: *llama.llama_context,

    const Self = @This();

    pub fn init(model_path: [*:0]const u8, params: llama.llama_context_params) !Self {
        const model = llama.llama_load_model_from_file(model_path, .{}) orelse
            return error.ModelLoadFailed;

        const ctx = llama.llama_new_context_with_model(model, params) orelse
            return error.ContextCreateFailed;

        return Self{
            .model = model,
            .ctx = ctx,
        };
    }

    pub fn generate(self: *Self, tokens: []const llama.llama_token, params: GenerateParams) ![]llama.llama_token {
        // トークン生成ループ
        var output = std.ArrayList(llama.llama_token).init(allocator);

        while (output.items.len < params.max_tokens) {
            const logits = llama.llama_get_logits(self.ctx);
            const next_token = self.sampleToken(logits, params);

            if (next_token == llama.llama_token_eos(self.model)) break;

            try output.append(next_token);
            _ = llama.llama_decode(self.ctx, .{ .token = next_token });
        }

        return output.toOwnedSlice();
    }

    pub fn deinit(self: *Self) void {
        llama.llama_free(self.ctx);
        llama.llama_free_model(self.model);
    }
};
```

---

## 8. モデル管理

### 8.1 モデル配布方針

> **重要**: Zylix はモデルファイルを配布しない。
>
> - **Zylix**: モデルをホスト・配布しない
> - **開発者**: アプリにモデルを同梱しない（ライセンス・サイズの問題）
> - **エンドユーザー**: 自己責任でモデルを入手し、ローカルに配置する

```
┌─────────────────────────────────────────────────────────────┐
│                    エンドユーザー                            │
│  1. 外部サイトからモデルをダウンロード                        │
│  2. 所定のディレクトリに配置                                 │
│  3. アプリでパスを指定して使用                               │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│  モデル入手先（ユーザーが自分でアクセス）                     │
│  - HuggingFace: https://huggingface.co/                     │
│  - Ollama: https://ollama.com/library                       │
│  - 各モデル公式サイト                                        │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│  ローカル配置先                                              │
│  - ~/.zylix/models/           (デフォルト)                  │
│  - アプリ指定の任意パス                                      │
└─────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────┐
│  Zylix AI (読み込みのみ)                                     │
│  - パス指定でモデルをロード                                  │
│  - フォーマット検証                                          │
│  - 推論実行                                                  │
└─────────────────────────────────────────────────────────────┘
```

### 8.2 推奨モデル入手先

| モデル種別 | 推奨入手先 | 備考 |
|-----------|-----------|------|
| LLM (GGUF) | [HuggingFace TheBloke](https://huggingface.co/TheBloke) | 量子化済みモデル多数 |
| LLM (GGUF) | [Ollama Library](https://ollama.com/library) | ollama pull 後に変換可能 |
| Embedding | [HuggingFace MTEB](https://huggingface.co/spaces/mteb/leaderboard) | ベンチマーク参照 |
| Whisper | [HuggingFace ggerganov](https://huggingface.co/ggerganov/whisper.cpp) | whisper.cpp 形式 |
| VLM | [HuggingFace Qwen](https://huggingface.co/Qwen) | Qwen2-VL 等 |

### 8.3 モデルマネージャー（ローカル管理のみ）

```typescript
// bindings/typescript/src/ai/model-manager.ts

export interface LocalModelInfo {
  path: string;
  name: string;
  type: 'embedding' | 'llm' | 'vlm' | 'whisper';
  size: number;        // bytes
  format: 'gguf' | 'onnx' | 'coreml' | 'tflite';
  lastAccessed: Date;
}

export class ModelManager {
  private modelsDir: string;

  constructor(modelsDir: string = '~/.zylix/models') {
    this.modelsDir = modelsDir;
  }

  // ローカルにあるモデル一覧を取得
  async listLocal(): Promise<LocalModelInfo[]> {
    const files = await fs.readdir(this.modelsDir);
    return files
      .filter(f => this.isSupportedFormat(f))
      .map(f => this.getModelInfo(path.join(this.modelsDir, f)));
  }

  // モデルの存在確認
  async exists(modelPath: string): Promise<boolean> {
    try {
      await fs.access(modelPath);
      return true;
    } catch {
      return false;
    }
  }

  // モデルフォーマット検証
  async validate(modelPath: string): Promise<ValidationResult> {
    if (!await this.exists(modelPath)) {
      return { valid: false, error: 'File not found' };
    }

    const format = this.detectFormat(modelPath);
    if (!format) {
      return { valid: false, error: 'Unsupported format' };
    }

    // ヘッダー検証（破損チェック）
    const isValid = await this.validateHeader(modelPath, format);
    if (!isValid) {
      return { valid: false, error: 'Invalid or corrupted model file' };
    }

    return { valid: true, format };
  }

  // モデル削除（ローカルファイルのみ）
  async delete(modelPath: string): Promise<void> {
    await fs.unlink(modelPath);
  }

  private isSupportedFormat(filename: string): boolean {
    return /\.(gguf|onnx|mlmodel|tflite)$/.test(filename);
  }

  private detectFormat(path: string): string | null {
    if (path.endsWith('.gguf')) return 'gguf';
    if (path.endsWith('.onnx')) return 'onnx';
    if (path.endsWith('.mlmodel')) return 'coreml';
    if (path.endsWith('.tflite')) return 'tflite';
    return null;
  }
}

interface ValidationResult {
  valid: boolean;
  format?: string;
  error?: string;
}
```

### 8.4 使用例

```typescript
import { ZylixLLM, ModelManager } from '@zylix/ai';

async function loadUserModel() {
  const manager = new ModelManager();

  // ユーザーが配置したモデルのパス
  const modelPath = '~/.zylix/models/qwen3-1.7b-q4_k_m.gguf';

  // 存在・フォーマット検証
  const validation = await manager.validate(modelPath);
  if (!validation.valid) {
    console.error('モデルが見つからないか、形式が不正です:', validation.error);
    console.log('以下からモデルをダウンロードしてください:');
    console.log('  https://huggingface.co/Qwen/Qwen3-1.7B-GGUF');
    return;
  }

  // モデルロード
  const llm = await ZylixLLM.load({
    modelPath,
    useGPU: true,
  });

  const response = await llm.generate('Hello, world!');
  console.log(response);

  await llm.dispose();
}
```

### 8.5 エラーハンドリング

```typescript
export class ModelNotFoundError extends Error {
  constructor(path: string, suggestions: string[]) {
    super(`Model not found: ${path}`);
    this.name = 'ModelNotFoundError';
    this.suggestions = suggestions;
  }

  suggestions: string[];

  // ユーザーへのガイダンスメッセージ生成
  getGuidance(): string {
    return [
      'モデルファイルが見つかりません。',
      '',
      '以下の手順でモデルを入手してください:',
      '1. 推奨入手先からモデルをダウンロード',
      ...this.suggestions.map(s => `   - ${s}`),
      '2. ダウンロードしたファイルを以下に配置',
      '   ~/.zylix/models/',
      '3. アプリを再起動',
    ].join('\n');
  }
}
```

---

## 9. 実装スケジュール

### Phase 1: 基盤 (2週間)
- [ ] GGML/llama.cpp のZigバインディング
- [ ] 基本的なモデルロード/アンロード
- [ ] メモリ管理フレームワーク

### Phase 2: 埋め込みモデル (2週間)
- [ ] Embedding API実装
- [ ] バッチ処理
- [ ] セマンティック検索サンプル

### Phase 3: LLM (3週間)
- [ ] テキスト生成API
- [ ] ストリーミング出力
- [ ] チャット形式対応

### Phase 4: VLM (2週間)
- [ ] 画像処理パイプライン
- [ ] VLM推論
- [ ] OCR機能

### Phase 5: Whisper (2週間)
- [ ] 音声処理
- [ ] 文字起こしAPI
- [ ] リアルタイム対応

### Phase 6: プラットフォーム最適化 (3週間)
- [ ] iOS Core ML統合
- [ ] Android NNAPI統合
- [ ] Web WebGPU統合

### Phase 7: テスト & ドキュメント (2週間)
- [ ] 単体テスト
- [ ] 統合テスト
- [ ] APIドキュメント
- [ ] サンプルアプリ

---

## 10. 依存関係

### 外部ライブラリ

| ライブラリ | バージョン | 用途 |
|-----------|-----------|------|
| llama.cpp | latest | LLM推論エンジン |
| whisper.cpp | latest | 音声認識 |
| ggml | latest | テンソル演算 |
| sentencepiece | latest | トークナイザー |

### プラットフォーム依存

| プラットフォーム | SDK/フレームワーク |
|-----------------|-------------------|
| iOS | Core ML, Metal, Accelerate |
| Android | TensorFlow Lite, NNAPI |
| Web | ONNX Runtime Web, WebGPU |
| macOS | Core ML, Metal |
| Windows | DirectML, ONNX Runtime |
| Linux | GGML, Vulkan (optional) |

---

## 11. メトリクス目標

| 指標 | 目標値 |
|------|--------|
| 埋め込み速度 | <50ms / 文 |
| LLM トークン生成 | >30 tokens/sec (Q4) |
| VLM 画像分析 | <3秒 / 画像 |
| Whisper 文字起こし | リアルタイムの2倍速以上 |
| メモリ使用量 | モデルサイズ + 20%以下 |
| 初回ロード時間 | <5秒 (Q4モデル) |
